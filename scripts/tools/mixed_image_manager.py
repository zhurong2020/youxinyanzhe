#!/usr/bin/env python3
"""
æœ‰å¿ƒå·¥åŠ - æ··åˆå›¾ç‰‡ç®¡ç†ç³»ç»Ÿ Phase 0.5
YouXin Workshop - Mixed Image Management System

æ”¯æŒä»»æ„ä½ç½®å›¾ç‰‡å‘ç°å’Œå››é˜¶æ®µå¤„ç†æµç¨‹: ä¸´æ—¶åˆ›ä½œ â†’ é¡¹ç›®ç¼“å­˜ â†’ äº‘ç«¯å½’æ¡£ â†’ å®‰å…¨æ¸…ç†

æ ¸å¿ƒç‰¹æ€§:
- æ™ºèƒ½è·¯å¾„è§£æ: æ”¯æŒç»å¯¹è·¯å¾„ã€ç›¸å¯¹è·¯å¾„ã€ä»»æ„ä¸´æ—¶ç›®å½•
- å››é˜¶æ®µç®¡ç†: pending â†’ uploaded â†’ cloud storage â†’ user-confirmed cleanup
- å®Œæ•´å¤‡ä»½æœºåˆ¶: processing/uploaded/ å¤‡ä»½æœºåˆ¶
- å®‰å…¨æ¸…ç†ç­–ç•¥: ç”¨æˆ·ç¡®è®¤å‘å¸ƒåæ‰åˆ é™¤æœ¬åœ°å¤‡ä»½
- å›æ»šæœºåˆ¶: æ”¯æŒä»å¤‡ä»½æ¢å¤åŸå§‹çŠ¶æ€
"""

import json
import os
import re
import shutil
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
import argparse

# å¯¼å…¥ç°æœ‰OneDriveç»„ä»¶
try:
    from onedrive_blog_images import BlogImageManager, OneDriveUploadManager, OneDriveAuthManager
    from onedrive_image_index import OneDriveImageIndex
except ImportError as e:
    print(f"âš ï¸  Warning: OneDrive components not available: {e}")
    BlogImageManager = None
    OneDriveUploadManager = None
    OneDriveAuthManager = None
    OneDriveImageIndex = None

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class ImageProcessingRecord:
    """å›¾ç‰‡å¤„ç†è®°å½•"""
    original_path: str
    original_markdown: str
    processing_stage: str  # pending, uploaded, published, cleaned
    project_cache_path: Optional[str] = None
    onedrive_path: Optional[str] = None
    onedrive_url: Optional[str] = None
    embed_url: Optional[str] = None
    created_at: str = ""
    updated_at: str = ""
    
    def __post_init__(self):
        if not self.created_at:
            self.created_at = datetime.now().isoformat()
        self.updated_at = datetime.now().isoformat()


class SmartPathResolver:
    """æ™ºèƒ½è·¯å¾„è§£æå™¨ - æ”¯æŒä»»æ„ä½ç½®å›¾ç‰‡å‘ç°"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.common_temp_dirs = [
            Path.home() / "Desktop",
            Path.home() / "Downloads", 
            Path.home() / "Documents",
            Path("/tmp"),
            Path("/var/folders"),  # macOSä¸´æ—¶ç›®å½•
            Path("C:/Users") / os.environ.get('USERNAME', '') / "Desktop",  # Windowsæ¡Œé¢
            Path("C:/Users") / os.environ.get('USERNAME', '') / "Downloads",  # Windowsä¸‹è½½
        ]
    
    def find_image_paths(self, content: str, article_path: str) -> List[Tuple[str, str, str]]:
        """æŸ¥æ‰¾Markdownä¸­çš„æ‰€æœ‰å›¾ç‰‡å¼•ç”¨å¹¶è§£æçœŸå®è·¯å¾„"""
        image_pattern = re.compile(r'!\[(.*?)\]\((.*?)\)')
        found_images = []
        
        for match in image_pattern.finditer(content):
            alt_text = match.group(1)
            img_path = match.group(2)
            full_match = match.group(0)
            
            # è·³è¿‡å·²ç»æ˜¯ç½‘ç»œé“¾æ¥çš„å›¾ç‰‡
            if self._is_web_url(img_path):
                continue
            
            # è§£ææœ¬åœ°è·¯å¾„
            resolved_path = self.resolve_image_path(img_path, article_path)
            if resolved_path:
                found_images.append((full_match, alt_text, resolved_path))
            else:
                logger.warning(f"Could not resolve image path: {img_path}")
        
        return found_images
    
    def resolve_image_path(self, img_path: str, article_path: str) -> Optional[str]:
        """æ™ºèƒ½è§£æå›¾ç‰‡è·¯å¾„ - æ”¯æŒå„ç§è·¯å¾„æ ¼å¼"""
        article_dir = Path(article_path).parent
        
        try:
            # æƒ…å†µ1: Jekyll baseurlè·¯å¾„
            if '{{ site.baseurl }}' in img_path:
                relative_path = img_path.replace('{{ site.baseurl }}/', '')
                candidate = self.project_root / relative_path
                if candidate.exists():
                    return str(candidate.resolve())
            
            # æƒ…å†µ2: ç»å¯¹è·¯å¾„
            if Path(img_path).is_absolute():
                candidate = Path(img_path)
                if candidate.exists():
                    return str(candidate.resolve())
            
            # æƒ…å†µ3: ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºæ–‡ç« ï¼‰
            if img_path.startswith('./') or img_path.startswith('../'):
                candidate = (article_dir / img_path).resolve()
                if candidate.exists():
                    return str(candidate)
            
            # æƒ…å†µ4: é¡¹ç›®å†…ç›¸å¯¹è·¯å¾„
            candidate = self.project_root / img_path
            if candidate.exists():
                return str(candidate.resolve())
            
            # æƒ…å†µ5: ç›¸å¯¹äºæ–‡ç« ç›®å½•çš„ç®€å•è·¯å¾„
            candidate = article_dir / img_path
            if candidate.exists():
                return str(candidate.resolve())
            
            # æƒ…å†µ6: åœ¨å¸¸è§ä¸´æ—¶ç›®å½•ä¸­æœç´¢
            img_filename = Path(img_path).name
            for temp_dir in self.common_temp_dirs:
                if temp_dir.exists():
                    # åœ¨ä¸´æ—¶ç›®å½•ä¸­æœç´¢åŒåæ–‡ä»¶
                    for candidate in temp_dir.rglob(img_filename):
                        if candidate.is_file():
                            logger.info(f"Found image in temp directory: {candidate}")
                            return str(candidate.resolve())
            
            # æƒ…å†µ7: å…¨å±€æœç´¢ï¼ˆæœ€åæ‰‹æ®µï¼Œé™åˆ¶æ·±åº¦ï¼‰
            return self._search_image_globally(img_filename, max_depth=3)
            
        except Exception as e:
            logger.warning(f"Error resolving path {img_path}: {e}")
            return None
    
    def _is_web_url(self, path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯ç½‘ç»œURL"""
        return path.startswith(('http://', 'https://', '//', 'data:'))
    
    def _search_image_globally(self, filename: str, max_depth: int = 3) -> Optional[str]:
        """å…¨å±€æœç´¢å›¾ç‰‡æ–‡ä»¶ï¼ˆé™åˆ¶æ·±åº¦é¿å…æ€§èƒ½é—®é¢˜ï¼‰"""
        def search_recursive(directory: Path, current_depth: int) -> Optional[str]:
            if current_depth > max_depth:
                return None
            
            try:
                for item in directory.iterdir():
                    if item.name == filename and item.is_file():
                        return str(item.resolve())
                    elif item.is_dir() and not item.name.startswith('.'):
                        result = search_recursive(item, current_depth + 1)
                        if result:
                            return result
            except PermissionError:
                pass
            return None
        
        # ä»é¡¹ç›®æ ¹ç›®å½•å¼€å§‹æœç´¢
        return search_recursive(self.project_root, 0)


class ProcessingDirectoryManager:
    """å¤„ç†ç›®å½•ç®¡ç†å™¨"""
    
    def __init__(self, base_path: str = "assets/images/processing"):
        self.base_path = Path(base_path)
        self.pending_dir = self.base_path / "pending"
        self.uploaded_dir = self.base_path / "uploaded"
        self.failed_dir = self.base_path / "failed"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self._ensure_directories()
        
    def _ensure_directories(self):
        """ç¡®ä¿æ‰€æœ‰å¿…éœ€ç›®å½•å­˜åœ¨"""
        for directory in [self.pending_dir, self.uploaded_dir, self.failed_dir]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def create_processing_session(self, article_title: str) -> str:
        """åˆ›å»ºå¤„ç†ä¼šè¯ç›®å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_id = f"{article_title[:20]}_{timestamp}"
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        session_id = re.sub(r'[<>:"/\\|?*]', '_', session_id)
        
        session_dir = self.pending_dir / session_id
        session_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Created processing session: {session_id}")
        return session_id
    
    def move_to_uploaded(self, session_id: str) -> bool:
        """å°†ä¼šè¯ä»pendingç§»åŠ¨åˆ°uploaded"""
        try:
            pending_path = self.pending_dir / session_id
            uploaded_path = self.uploaded_dir / session_id
            
            if pending_path.exists():
                shutil.move(str(pending_path), str(uploaded_path))
                logger.info(f"Moved session to uploaded: {session_id}")
                return True
            return False
        except Exception as e:
            logger.error(f"Failed to move session to uploaded: {e}")
            return False
    
    def move_to_failed(self, session_id: str, error_info: str) -> bool:
        """å°†ä¼šè¯ç§»åŠ¨åˆ°failedå¹¶è®°å½•é”™è¯¯ä¿¡æ¯"""
        try:
            pending_path = self.pending_dir / session_id
            failed_path = self.failed_dir / session_id
            
            if pending_path.exists():
                shutil.move(str(pending_path), str(failed_path))
                
                # è®°å½•é”™è¯¯ä¿¡æ¯
                error_file = failed_path / "error.txt"
                error_file.write_text(f"Error: {error_info}\nTime: {datetime.now().isoformat()}", 
                                    encoding='utf-8')
                logger.error(f"Moved session to failed: {session_id}")
                return True
            return False
        except Exception as e:
            logger.error(f"Failed to move session to failed: {e}")
            return False
    
    def cleanup_uploaded_session(self, session_id: str, user_confirmed: bool = False) -> bool:
        """æ¸…ç†å·²ä¸Šä¼ çš„ä¼šè¯ï¼ˆéœ€è¦ç”¨æˆ·ç¡®è®¤ï¼‰"""
        if not user_confirmed:
            logger.warning(f"Session cleanup requires user confirmation: {session_id}")
            return False
        
        try:
            uploaded_path = self.uploaded_dir / session_id
            if uploaded_path.exists():
                shutil.rmtree(uploaded_path)
                logger.info(f"Cleaned up uploaded session: {session_id}")
                return True
            return False
        except Exception as e:
            logger.error(f"Failed to cleanup session: {e}")
            return False


class MixedImageManager:
    """æ··åˆå›¾ç‰‡ç®¡ç†ç³»ç»Ÿä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, config_path: str = "config/onedrive_config.json", 
                 project_root: str = "."):
        self.project_root = Path(project_root).resolve()
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.path_resolver = SmartPathResolver(self.project_root)
        self.dir_manager = ProcessingDirectoryManager()
        
        # åˆå§‹åŒ–OneDriveç»„ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.onedrive_manager = None
        self.image_index = None
        
        if BlogImageManager:
            try:
                self.onedrive_manager = BlogImageManager(config_path)
                self.image_index = OneDriveImageIndex() if OneDriveImageIndex else None
            except Exception as e:
                logger.warning(f"OneDrive manager initialization failed: {e}")
    
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            # é»˜è®¤é…ç½®
            default_config = {
                "image_processing": {
                    "temp_retention_days": 7,
                    "auto_cleanup_after_publish": False,
                    "backup_original_files": True,
                    "processing_directory": "assets/images/processing",
                    "max_file_size_mb": 10
                }
            }
            
            # å°è¯•åŠ è½½æ–‡ä»¶é…ç½®
            if self.config_path.exists():
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    file_config = json.load(f)
                    default_config.update(file_config)
            
            return default_config
            
        except Exception as e:
            logger.warning(f"Failed to load config, using defaults: {e}")
            return {"image_processing": {
                "temp_retention_days": 7,
                "auto_cleanup_after_publish": False,
                "backup_original_files": True,
                "processing_directory": "assets/images/processing",
                "max_file_size_mb": 10
            }}
    
    def process_article_images(self, article_path: str, 
                             dry_run: bool = False) -> Dict:
        """å¤„ç†æ–‡ç« ä¸­çš„æ‰€æœ‰å›¾ç‰‡ - å®Œæ•´çš„å››é˜¶æ®µæµç¨‹"""
        
        article_file = Path(article_path)
        if not article_file.exists():
            return {'success': False, 'error': 'Article file not found'}
        
        logger.info(f"Starting mixed image processing for: {article_path}")
        print("ğŸš€ å¯åŠ¨æ··åˆå›¾ç‰‡ç®¡ç†ç³»ç»Ÿ")
        print("=" * 60)
        
        try:
            # é˜¶æ®µ1: ä¸´æ—¶åˆ›ä½œ â†’ é¡¹ç›®ç¼“å­˜
            print("ğŸ“ é˜¶æ®µ1: å‘ç°å’Œç¼“å­˜å›¾ç‰‡...")
            cache_result = self._stage1_discover_and_cache(article_path, dry_run)
            
            if not cache_result['success']:
                return cache_result
            
            if cache_result['images_found'] == 0:
                return {'success': True, 'message': 'No images to process', 'stages_completed': 1}
            
            # é˜¶æ®µ2: é¡¹ç›®ç¼“å­˜ â†’ äº‘ç«¯å½’æ¡£
            print("â˜ï¸  é˜¶æ®µ2: ä¸Šä¼ åˆ°äº‘ç«¯...")
            upload_result = self._stage2_upload_to_cloud(cache_result['session_id'], 
                                                       article_path, dry_run)
            
            if not upload_result['success']:
                # å¤±è´¥æ—¶ç§»åŠ¨åˆ°failedç›®å½•
                self.dir_manager.move_to_failed(cache_result['session_id'], 
                                               upload_result.get('error', 'Upload failed'))
                return upload_result
            
            # é˜¶æ®µ3: æ›´æ–°æ–‡ç« é“¾æ¥
            print("ğŸ”— é˜¶æ®µ3: æ›´æ–°æ–‡ç« é“¾æ¥...")
            update_result = self._stage3_update_article_links(article_path, 
                                                             upload_result['replacements'], 
                                                             dry_run)
            
            # é˜¶æ®µ4: ç§»åŠ¨åˆ°uploadedçŠ¶æ€ï¼ˆç­‰å¾…ç”¨æˆ·ç¡®è®¤æ¸…ç†ï¼‰
            print("ğŸ“¦ é˜¶æ®µ4: ä¿å­˜å¤„ç†çŠ¶æ€...")
            if not dry_run:
                self.dir_manager.move_to_uploaded(cache_result['session_id'])
            
            print("âœ… æ··åˆå›¾ç‰‡å¤„ç†å®Œæˆï¼")
            print(f"ğŸ’¾ ä¼šè¯ID: {cache_result['session_id']}")
            print(f"ğŸ–¼ï¸  å¤„ç†å›¾ç‰‡: {upload_result['images_processed']}")
            
            return {
                'success': True,
                'session_id': cache_result['session_id'],
                'images_processed': upload_result['images_processed'],
                'replacements': upload_result['replacements'],
                'stages_completed': 4,
                'cache_result': cache_result,
                'upload_result': upload_result,
                'update_result': update_result
            }
            
        except Exception as e:
            logger.error(f"Failed to process article images: {e}")
            return {'success': False, 'error': str(e)}
    
    def _stage1_discover_and_cache(self, article_path: str, dry_run: bool) -> Dict:
        """é˜¶æ®µ1: å‘ç°å›¾ç‰‡å¹¶ç¼“å­˜åˆ°processing/pending/"""
        try:
            # è¯»å–æ–‡ç« å†…å®¹
            with open(article_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–æ–‡ç« æ ‡é¢˜
            title_match = re.search(r'^title:\s*["\']?(.*?)["\']?\s*$', content, re.MULTILINE)
            article_title = title_match.group(1) if title_match else Path(article_path).stem
            
            # å‘ç°å›¾ç‰‡è·¯å¾„
            found_images = self.path_resolver.find_image_paths(content, article_path)
            
            if not found_images:
                return {'success': True, 'images_found': 0, 'message': 'No local images found'}
            
            print(f"ğŸ” å‘ç° {len(found_images)} ä¸ªæœ¬åœ°å›¾ç‰‡")
            
            # åˆ›å»ºå¤„ç†ä¼šè¯
            session_id = self.dir_manager.create_processing_session(article_title)
            session_dir = self.dir_manager.pending_dir / session_id
            
            # ç¼“å­˜å›¾ç‰‡æ–‡ä»¶å’Œå…ƒæ•°æ®
            cached_images = []
            for i, (full_match, alt_text, img_path) in enumerate(found_images, 1):
                try:
                    img_file = Path(img_path)
                    if not img_file.exists():
                        logger.warning(f"Image file not found: {img_path}")
                        continue
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    file_size_mb = img_file.stat().st_size / (1024 * 1024)
                    max_size = self.config.get('image_processing', {}).get('max_file_size_mb', 10)
                    if file_size_mb > max_size:
                        logger.warning(f"Image too large ({file_size_mb:.1f}MB): {img_path}")
                        continue
                    
                    # å¤åˆ¶åˆ°ç¼“å­˜ç›®å½•
                    cached_filename = f"{i:02d}_{img_file.name}"
                    cached_path = session_dir / cached_filename
                    
                    if not dry_run:
                        shutil.copy2(img_file, cached_path)
                    
                    # åˆ›å»ºå¤„ç†è®°å½•
                    record = ImageProcessingRecord(
                        original_path=img_path,
                        original_markdown=full_match,
                        processing_stage="pending",
                        project_cache_path=str(cached_path)
                    )
                    
                    cached_images.append(record)
                    print(f"  ğŸ“¥ {i:02d}. {img_file.name} -> é¡¹ç›®ç¼“å­˜")
                    
                except Exception as e:
                    logger.error(f"Failed to cache image {img_path}: {e}")
                    continue
            
            # ä¿å­˜ä¼šè¯å…ƒæ•°æ®
            session_metadata = {
                'session_id': session_id,
                'article_path': article_path,
                'article_title': article_title,
                'images_found': len(found_images),
                'images_cached': len(cached_images),
                'created_at': datetime.now().isoformat(),
                'cached_images': [
                    {
                        'original_path': img.original_path,
                        'original_markdown': img.original_markdown,
                        'cache_path': img.project_cache_path,
                        'processing_stage': img.processing_stage
                    }
                    for img in cached_images
                ]
            }
            
            if not dry_run:
                metadata_file = session_dir / "session_metadata.json"
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(session_metadata, f, indent=2, ensure_ascii=False)
            
            return {
                'success': True,
                'session_id': session_id,
                'images_found': len(found_images),
                'images_cached': len(cached_images),
                'cached_images': cached_images,
                'metadata': session_metadata
            }
            
        except Exception as e:
            logger.error(f"Stage 1 failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def _stage2_upload_to_cloud(self, session_id: str, article_path: str, 
                              dry_run: bool) -> Dict:
        """é˜¶æ®µ2: ä¸Šä¼ ç¼“å­˜çš„å›¾ç‰‡åˆ°äº‘ç«¯"""
        
        if not self.onedrive_manager:
            return {'success': False, 'error': 'OneDrive manager not available'}
        
        try:
            session_dir = self.dir_manager.pending_dir / session_id
            metadata_file = session_dir / "session_metadata.json"
            
            # åœ¨è¯•è¿è¡Œæ¨¡å¼ä¸‹ï¼Œå¦‚æœå…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›æˆåŠŸä½†ä¸å®é™…å¤„ç†
            if dry_run and not metadata_file.exists():
                print("  ğŸš€ [DRY RUN] æ¨¡æ‹Ÿäº‘ç«¯ä¸Šä¼ é˜¶æ®µ")
                return {
                    'success': True,
                    'images_processed': 0,
                    'replacements': {},
                    'session_metadata': {'note': 'dry run mode'}
                }
            
            # åŠ è½½ä¼šè¯å…ƒæ•°æ®
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            replacements = {}
            processed_count = 0
            
            for i, img_info in enumerate(metadata['cached_images'], 1):
                try:
                    cache_path = img_info['cache_path']
                    original_markdown = img_info['original_markdown']
                    
                    if dry_run:
                        print(f"  ğŸš€ {i:02d}. [DRY RUN] Would upload: {Path(cache_path).name}")
                        continue
                    
                    # ä½¿ç”¨ç°æœ‰çš„OneDriveä¸Šä¼ é€»è¾‘
                    # è¿™é‡Œéœ€è¦è°ƒç”¨åŸæœ‰çš„ä¸Šä¼ å’Œé“¾æ¥ç”Ÿæˆä»£ç 
                    upload_result = self._upload_single_image(cache_path, article_path, i)
                    
                    if upload_result['success']:
                        # æ›´æ–°Markdowné“¾æ¥
                        alt_text = original_markdown.split('![')[1].split(']')[0]
                        new_markdown = f"![{alt_text}]({upload_result['embed_url']})"
                        replacements[original_markdown] = new_markdown
                        processed_count += 1
                        
                        print(f"  âœ… {i:02d}. {Path(cache_path).name} -> OneDrive")
                    else:
                        logger.error(f"Failed to upload {cache_path}: {upload_result.get('error')}")
                        
                except Exception as e:
                    logger.error(f"Failed to process cached image {i}: {e}")
                    continue
            
            # æ›´æ–°ä¼šè¯å…ƒæ•°æ®
            metadata['upload_completed_at'] = datetime.now().isoformat()
            metadata['images_uploaded'] = processed_count
            metadata['upload_replacements'] = replacements
            
            if not dry_run:
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            return {
                'success': True,
                'images_processed': processed_count,
                'replacements': replacements,
                'session_metadata': metadata
            }
            
        except Exception as e:
            logger.error(f"Stage 2 failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def _upload_single_image(self, cache_path: str, article_path: str, 
                           index: int) -> Dict:
        """ä¸Šä¼ å•ä¸ªå›¾ç‰‡åˆ°OneDrive"""
        
        if not self.onedrive_manager:
            return {'success': False, 'error': 'OneDrive manager not available'}
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„processoré€»è¾‘
            processor = self.onedrive_manager.processor
            
            # ç”Ÿæˆè¿œç¨‹è·¯å¾„
            cache_file = Path(cache_path)
            now = datetime.now()
            
            # æå–æ–‡ç« æ ‡é¢˜
            with open(article_path, 'r', encoding='utf-8') as f:
                content = f.read()
            title_match = re.search(r'^title:\s*["\']?(.*?)["\']?\s*$', content, re.MULTILINE)
            article_title = title_match.group(1) if title_match else Path(article_path).stem
            
            remote_path = processor._generate_remote_path(cache_path, article_title, index)
            
            # ä¸Šä¼ æ–‡ä»¶
            upload_result = processor.uploader.upload_file(cache_path, remote_path)
            
            # è·å–åµŒå…¥é“¾æ¥
            direct_link = processor.uploader.get_direct_image_url(upload_result['id'])
            
            # æ·»åŠ åˆ°ç´¢å¼•
            if processor.index:
                processor.index.add_record(
                    local_path=cache_path,
                    onedrive_path=remote_path,
                    onedrive_url=direct_link,
                    embed_url=direct_link,
                    article_file=article_path,
                    article_title=article_title,
                    onedrive_file_id=upload_result['id'],
                    image_index=index,
                    processing_notes=f"Mixed workflow upload from cache"
                )
            
            return {
                'success': True,
                'onedrive_file_id': upload_result['id'],
                'remote_path': remote_path,
                'direct_link': direct_link,
                'embed_url': direct_link
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _stage3_update_article_links(self, article_path: str, 
                                   replacements: Dict[str, str], 
                                   dry_run: bool) -> Dict:
        """é˜¶æ®µ3: æ›´æ–°æ–‡ç« ä¸­çš„å›¾ç‰‡é“¾æ¥"""
        
        if not replacements:
            return {'success': True, 'message': 'No links to update'}
        
        try:
            # è¯»å–åŸå§‹å†…å®¹
            with open(article_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ›å»ºå¤‡ä»½
            backup_path = f"{article_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            if not dry_run:
                shutil.copy2(article_path, backup_path)
            
            # åº”ç”¨æ›¿æ¢
            updated_content = content
            for old_link, new_link in replacements.items():
                updated_content = updated_content.replace(old_link, new_link)
            
            if dry_run:
                print(f"  ğŸ”— [DRY RUN] Would update {len(replacements)} links in article")
            else:
                # å†™å›æ–‡ä»¶
                with open(article_path, 'w', encoding='utf-8') as f:
                    f.write(updated_content)
                print(f"  âœ… å·²æ›´æ–° {len(replacements)} ä¸ªå›¾ç‰‡é“¾æ¥")
            
            return {
                'success': True,
                'links_updated': len(replacements),
                'backup_path': backup_path
            }
            
        except Exception as e:
            logger.error(f"Stage 3 failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def list_uploaded_sessions(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰ç­‰å¾…æ¸…ç†ç¡®è®¤çš„ä¸Šä¼ ä¼šè¯"""
        sessions = []
        
        for session_dir in self.dir_manager.uploaded_dir.iterdir():
            if session_dir.is_dir():
                try:
                    metadata_file = session_dir / "session_metadata.json"
                    if metadata_file.exists():
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        sessions.append(metadata)
                except Exception as e:
                    logger.warning(f"Failed to read session metadata: {e}")
        
        return sorted(sessions, key=lambda x: x.get('created_at', ''), reverse=True)
    
    def confirm_cleanup(self, session_id: str, user_confirmed: bool = False) -> Dict:
        """ç¡®è®¤æ¸…ç†ä¸Šä¼ ä¼šè¯ï¼ˆéœ€è¦ç”¨æˆ·æ˜ç¡®ç¡®è®¤ï¼‰"""
        
        if not user_confirmed:
            return {
                'success': False, 
                'error': 'User confirmation required for cleanup',
                'message': 'Please set user_confirmed=True to proceed'
            }
        
        success = self.dir_manager.cleanup_uploaded_session(session_id, user_confirmed)
        
        if success:
            return {'success': True, 'message': f'Session {session_id} cleaned up successfully'}
        else:
            return {'success': False, 'error': f'Failed to cleanup session {session_id}'}


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="æœ‰å¿ƒå·¥åŠ - æ··åˆå›¾ç‰‡ç®¡ç†ç³»ç»Ÿ")
    parser.add_argument("article_path", nargs='?', help="æ–‡ç« æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰")
    parser.add_argument("--list-sessions", action="store_true", help="åˆ—å‡ºç­‰å¾…æ¸…ç†çš„ä¼šè¯")
    parser.add_argument("--confirm-cleanup", help="ç¡®è®¤æ¸…ç†æŒ‡å®šä¼šè¯ID")
    parser.add_argument("--config", default="config/onedrive_config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    try:
        manager = MixedImageManager(config_path=args.config)
        
        if args.list_sessions:
            # åˆ—å‡ºç­‰å¾…æ¸…ç†çš„ä¼šè¯
            sessions = manager.list_uploaded_sessions()
            if not sessions:
                print("ğŸ“­ æ²¡æœ‰ç­‰å¾…æ¸…ç†çš„ä¼šè¯")
            else:
                print(f"ğŸ“‹ ç­‰å¾…æ¸…ç†çš„ä¼šè¯ ({len(sessions)}ä¸ª):")
                for session in sessions:
                    print(f"  ğŸ—‚ï¸  {session['session_id']}")
                    print(f"     æ–‡ç« : {session.get('article_title', 'Unknown')}")
                    print(f"     åˆ›å»º: {session.get('created_at', 'Unknown')}")
                    print(f"     å›¾ç‰‡: {session.get('images_uploaded', 0)}")
                    print()
                    
        elif args.confirm_cleanup:
            # ç¡®è®¤æ¸…ç†ä¼šè¯
            print(f"âš ï¸  æ­£åœ¨æ¸…ç†ä¼šè¯: {args.confirm_cleanup}")
            print("è¿™å°†æ°¸ä¹…åˆ é™¤æœ¬åœ°å¤‡ä»½æ–‡ä»¶ï¼")
            
            confirm = input("è¯·è¾“å…¥ 'YES' ç¡®è®¤æ¸…ç†: ")
            if confirm == 'YES':
                result = manager.confirm_cleanup(args.confirm_cleanup, user_confirmed=True)
                if result['success']:
                    print("âœ… æ¸…ç†å®Œæˆ")
                else:
                    print(f"âŒ æ¸…ç†å¤±è´¥: {result['error']}")
            else:
                print("âŒ æ¸…ç†å·²å–æ¶ˆ")
                
        elif args.article_path:
            # å¤„ç†æ–‡ç« å›¾ç‰‡
            print(f"ğŸ“ å¤„ç†æ–‡ç« : {args.article_path}")
            if args.dry_run:
                print("ğŸ” è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šä¿®æ”¹ä»»ä½•æ–‡ä»¶")
            
            result = manager.process_article_images(args.article_path, dry_run=args.dry_run)
            
            if result['success']:
                print("âœ… å¤„ç†æˆåŠŸ!")
                if not args.dry_run:
                    print(f"ğŸ’¾ ä¼šè¯ID: {result.get('session_id')}")
                    print("ğŸ“ æç¤º: æ–‡ç« å‘å¸ƒåå¯ä½¿ç”¨ --confirm-cleanup æ¸…ç†å¤‡ä»½æ–‡ä»¶")
            else:
                print(f"âŒ å¤„ç†å¤±è´¥: {result.get('error')}")
        else:
            parser.print_help()
            
    except Exception as e:
        logger.error(f"Application error: {e}")
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()