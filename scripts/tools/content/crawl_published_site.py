#!/usr/bin/env python3
"""
åšå®¢ç½‘ç«™çˆ¬è™«å·¥å…·

åŠŸèƒ½ï¼š
1. çˆ¬å–zhurong2020.github.io/youxinyanzheç½‘ç«™ä¸Šçš„æ‰€æœ‰æ–‡ç« 
2. ä¸æœ¬åœ°_postsç›®å½•å¯¹æ¯”ï¼Œè¯†åˆ«ç¼ºå¤±æ–‡ç« 
3. ç”Ÿæˆå®Œæ•´çš„æ–‡ç« æ¸…å•

ä½¿ç”¨ï¼š
python scripts/tools/content/crawl_published_site.py
"""

import requests
from bs4 import BeautifulSoup
import re
from pathlib import Path
from datetime import datetime
import json


class BlogCrawler:
    def __init__(self):
        self.base_url = "https://zhurong2020.github.io"
        self.project_root = Path(__file__).parent.parent.parent.parent
        self.posts_dir = self.project_root / "_posts"
        self.online_articles = []
        self.local_articles = []

    def fetch_page(self, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def parse_posts_page(self, html):
        """è§£ææ–‡ç« å½’æ¡£é¡µé¢"""
        soup = BeautifulSoup(html, 'html.parser')
        articles = []

        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« æ¡ç›®
        # Jekyll Chirpyä¸»é¢˜é€šå¸¸ä½¿ç”¨ç‰¹å®šçš„HTMLç»“æ„
        post_items = soup.find_all(['article', 'div'], class_=re.compile(r'post|card'))

        if not post_items:
            # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
            post_items = soup.find_all('a', href=re.compile(r'/posts/\d{4}/\d{2}/'))

        for item in post_items:
            # æå–æ ‡é¢˜
            title_elem = item.find(['h1', 'h2', 'h3', 'h4'])
            if not title_elem:
                title_elem = item.find('a', href=re.compile(r'/posts/'))

            if not title_elem:
                continue

            title = title_elem.get_text(strip=True)

            # æå–é“¾æ¥
            link_elem = item.find('a', href=re.compile(r'/posts/'))
            if not link_elem:
                continue

            link = link_elem.get('href', '')
            if not link.startswith('http'):
                link = self.base_url + link

            # ä»é“¾æ¥ä¸­æå–slugå’Œæ—¥æœŸ
            match = re.search(r'/posts/(\d{4})/(\d{2})/([^/]+)/?$', link)
            if match:
                year, month, slug = match.groups()

                # æå–æ—¥æœŸï¼ˆå°è¯•ä»é¡µé¢æˆ–ä½¿ç”¨URLä¸­çš„æ—¥æœŸï¼‰
                date_elem = item.find(class_=re.compile(r'date|time'))
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    # å°è¯•è§£ææ—¥æœŸ
                    for fmt in ['%B %d, %Y', '%Y-%m-%d', '%d %B %Y']:
                        try:
                            date_obj = datetime.strptime(date_text, fmt)
                            date = date_obj.strftime('%Y-%m-%d')
                            break
                        except:
                            continue
                    else:
                        date = f"{year}-{month}-01"
                else:
                    date = f"{year}-{month}-01"

                articles.append({
                    'title': title,
                    'link': link,
                    'slug': slug,
                    'date': date,
                    'year': year,
                    'month': month
                })

        return articles

    def crawl_online_articles(self):
        """çˆ¬å–çº¿ä¸Šæ‰€æœ‰æ–‡ç« """
        print("ğŸŒ å¼€å§‹çˆ¬å–çº¿ä¸Šåšå®¢æ–‡ç« ...")

        # 1. çˆ¬å–æ–‡ç« å½’æ¡£é¡µé¢
        posts_url = f"{self.base_url}/posts/"
        html = self.fetch_page(posts_url)
        if html:
            articles = self.parse_posts_page(html)
            self.online_articles.extend(articles)
            print(f"âœ… ä»å½’æ¡£é¡µé¢è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        # 2. çˆ¬å–é¦–é¡µ
        home_url = self.base_url
        html = self.fetch_page(home_url)
        if html:
            articles = self.parse_posts_page(html)
            # å»é‡
            existing_slugs = {a['slug'] for a in self.online_articles}
            new_articles = [a for a in articles if a['slug'] not in existing_slugs]
            self.online_articles.extend(new_articles)
            print(f"âœ… ä»é¦–é¡µé¢å¤–è·å– {len(new_articles)} ç¯‡æ–‡ç« ")

        # å»é‡å¹¶æ’åº
        unique_articles = {}
        for article in self.online_articles:
            unique_articles[article['slug']] = article

        self.online_articles = list(unique_articles.values())
        self.online_articles.sort(key=lambda x: x['date'], reverse=True)

        print(f"ğŸ“Š çº¿ä¸Šæ–‡ç« æ€»æ•°: {len(self.online_articles)} ç¯‡")

    def scan_local_articles(self):
        """æ‰«ææœ¬åœ°æ–‡ç« """
        print("\nğŸ“ æ‰«ææœ¬åœ°_postsç›®å½•...")

        for md_file in self.posts_dir.glob("*.md"):
            filename = md_file.stem
            match = re.match(r'^(\d{4}-\d{2}-\d{2})-(.+)$', filename)
            if match:
                date, slug = match.groups()
                self.local_articles.append({
                    'filename': md_file.name,
                    'slug': slug,
                    'date': date
                })

        print(f"âœ… æœ¬åœ°æ–‡ç« æ€»æ•°: {len(self.local_articles)} ç¯‡")

    def compare_articles(self):
        """å¯¹æ¯”çº¿ä¸Šå’Œæœ¬åœ°æ–‡ç« """
        print("\nğŸ” å¯¹æ¯”çº¿ä¸Šå’Œæœ¬åœ°æ–‡ç« ...")

        local_slugs = {a['slug'] for a in self.local_articles}
        online_slugs = {a['slug'] for a in self.online_articles}

        # çº¿ä¸Šæœ‰ä½†æœ¬åœ°æ²¡æœ‰çš„æ–‡ç« 
        missing_local = []
        for article in self.online_articles:
            if article['slug'] not in local_slugs:
                missing_local.append(article)

        # æœ¬åœ°æœ‰ä½†çº¿ä¸Šæ²¡æœ‰çš„æ–‡ç« ï¼ˆå¯èƒ½æ˜¯è‰ç¨¿ï¼‰
        not_published = []
        for article in self.local_articles:
            if article['slug'] not in online_slugs:
                not_published.append(article)

        return missing_local, not_published

    def generate_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        missing_local, not_published = self.compare_articles()

        print("\n" + "="*60)
        print("ğŸ“Š æ–‡ç« å¯¹æ¯”æŠ¥å‘Š")
        print("="*60)

        print(f"\nçº¿ä¸Šæ–‡ç« æ€»æ•°: {len(self.online_articles)} ç¯‡")
        print(f"æœ¬åœ°æ–‡ç« æ€»æ•°: {len(self.local_articles)} ç¯‡")

        if missing_local:
            print(f"\nâš ï¸ çº¿ä¸Šæœ‰ä½†æœ¬åœ°ç¼ºå¤±çš„æ–‡ç« : {len(missing_local)} ç¯‡")
            for article in missing_local:
                print(f"  - {article['date']} | {article['title']}")
                print(f"    é“¾æ¥: {article['link']}")
        else:
            print("\nâœ… æ‰€æœ‰çº¿ä¸Šæ–‡ç« åœ¨æœ¬åœ°éƒ½æœ‰å¯¹åº”æ–‡ä»¶")

        if not_published:
            print(f"\nğŸ“ æœ¬åœ°æœ‰ä½†æœªå‘å¸ƒçš„æ–‡ç« : {len(not_published)} ç¯‡")
            for article in not_published:
                print(f"  - {article['date']} | {article['slug']}")

        print("\n" + "="*60)

        # å¯¼å‡ºå®Œæ•´æ–‡ç« åˆ—è¡¨
        output = {
            'online_articles': self.online_articles,
            'local_articles': self.local_articles,
            'missing_local': missing_local,
            'not_published': not_published,
            'generated_at': datetime.now().isoformat()
        }

        output_file = self.project_root / "_drafts" / "todos" / "crawled_articles.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

        return missing_local, not_published

    def generate_markdown_list(self):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æ–‡ç« åˆ—è¡¨"""
        output = []
        output.append("## ğŸŒ çº¿ä¸Šå‘å¸ƒçš„æ–‡ç« å®Œæ•´æ¸…å•\n")
        output.append(f"**çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        output.append(f"**æ–‡ç« æ€»æ•°**: {len(self.online_articles)} ç¯‡\n")

        # æŒ‰å¹´ä»½åˆ†ç»„
        articles_by_year = {}
        for article in self.online_articles:
            year = article['year']
            if year not in articles_by_year:
                articles_by_year[year] = []
            articles_by_year[year].append(article)

        for year in sorted(articles_by_year.keys(), reverse=True):
            articles = articles_by_year[year]
            output.append(f"### {year}å¹´ ({len(articles)}ç¯‡)\n")
            output.append("| å‘å¸ƒæ—¥æœŸ | æ–‡ç« æ ‡é¢˜ | é“¾æ¥ |")
            output.append("|---------|---------|------|")

            for article in sorted(articles, key=lambda x: x['date'], reverse=True):
                date = article['date']
                title = article['title']
                link = article['link']
                output.append(f"| {date} | {title} | [æŸ¥çœ‹]({link}) |")

            output.append("")

        return "\n".join(output)

    def run(self):
        """æ‰§è¡Œçˆ¬å–å’Œå¯¹æ¯”"""
        self.crawl_online_articles()
        self.scan_local_articles()
        missing_local, not_published = self.generate_report()

        # ç”ŸæˆMarkdownåˆ—è¡¨
        markdown_list = self.generate_markdown_list()

        output_file = self.project_root / "_drafts" / "todos" / "online_articles_list.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_list)

        print(f"\nğŸ“„ Markdownæ–‡ç« åˆ—è¡¨å·²ä¿å­˜åˆ°: {output_file}")


def main():
    crawler = BlogCrawler()
    crawler.run()


if __name__ == "__main__":
    main()
