#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å»é‡åˆ†æå·¥å…· - ç²¾ç¡®åŒ¹é…Grideaå’ŒJekyll(_posts)æ–‡ç« 
"""

import os
import re
import yaml
from datetime import datetime
from pathlib import Path

class ArticleDeduplicator:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent.parent
        self.gridea_dir = Path("/mnt/c/onedrive/wuxiami/OneDrive/Documents/Gridea/posts")
        self.jekyll_dir = self.project_root / "_posts"

        self.gridea_articles = []
        self.jekyll_articles = []

    def extract_front_matter(self, file_path, source='jekyll'):
        """æå–front matter"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # åŒ¹é…YAML front matter
            match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
            if not match:
                return None

            front_matter = yaml.safe_load(match.group(1))

            # æå–å…³é”®ä¿¡æ¯
            article = {
                'title': front_matter.get('title', 'æ— æ ‡é¢˜'),
                'file_path': str(file_path),
                'source': source
            }

            # å¤„ç†æ—¥æœŸ
            date_value = front_matter.get('date', front_matter.get('published', ''))
            if isinstance(date_value, datetime):
                article['date'] = date_value.strftime('%Y-%m-%d')
            else:
                article['date'] = str(date_value)[:10] if date_value else ''

            # ç»Ÿè®¡å­—æ•°
            body = content[match.end():].strip()
            article['word_count'] = len(re.sub(r'\s+', '', body))

            # æå–æ ‡ç­¾å’Œåˆ†ç±»
            article['tags'] = front_matter.get('tags', [])
            article['category'] = front_matter.get('category', '')

            return article

        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def scan_gridea_articles(self):
        """æ‰«æGrideaæ–‡ç« """
        print(f"ğŸ“‚ æ‰«æGrideaç›®å½•: {self.gridea_dir}")

        for file_path in self.gridea_dir.glob("*.md"):
            if file_path.name == 'about.md':
                continue

            article = self.extract_front_matter(file_path, source='Gridea')
            if article:
                self.gridea_articles.append(article)

        print(f"âœ… Grideaæ–‡ç« : {len(self.gridea_articles)} ç¯‡")

    def scan_jekyll_articles(self):
        """æ‰«æJekyll(_posts)æ–‡ç« """
        print(f"ğŸ“‚ æ‰«æJekyllç›®å½•: {self.jekyll_dir}")

        for file_path in self.jekyll_dir.glob("*.md"):
            article = self.extract_front_matter(file_path, source='Jekyll')
            if article:
                self.jekyll_articles.append(article)

        print(f"âœ… Jekyllæ–‡ç« : {len(self.jekyll_articles)} ç¯‡")

    def normalize_title(self, title):
        """æ ‡å‡†åŒ–æ ‡é¢˜ç”¨äºåŒ¹é…"""
        # å»é™¤ç©ºæ ¼ã€æ ‡ç‚¹ã€emojiç­‰ï¼Œè½¬å°å†™
        normalized = re.sub(r'[^\w\u4e00-\u9fff]+', '', title.lower())
        return normalized

    def find_duplicates(self):
        """æŸ¥æ‰¾é‡å¤æ–‡ç« """
        # åˆ›å»ºJekyllæ–‡ç« æ ‡é¢˜ç´¢å¼•
        jekyll_title_map = {}
        for article in self.jekyll_articles:
            normalized = self.normalize_title(article['title'])
            jekyll_title_map[normalized] = article

        print(f"\nğŸ” å¼€å§‹å»é‡åŒ¹é…...")
        print(f"Jekyllæ–‡ç« æ ‡é¢˜ç´¢å¼•: {len(jekyll_title_map)} æ¡")

        # åŒ¹é…Grideaæ–‡ç« 
        duplicates = []
        unique_gridea = []

        for gridea_article in self.gridea_articles:
            normalized = self.normalize_title(gridea_article['title'])

            if normalized in jekyll_title_map:
                jekyll_match = jekyll_title_map[normalized]
                duplicates.append({
                    'gridea': gridea_article,
                    'jekyll': jekyll_match
                })
            else:
                unique_gridea.append(gridea_article)

        return duplicates, unique_gridea

    def categorize_article(self, article):
        """æ ¹æ®æ ‡é¢˜å’Œæ ‡ç­¾åˆ†ç±»æ–‡ç« """
        title = article['title'].lower()
        tags = [t.lower() for t in article.get('tags', [])]
        category = article.get('category', '').lower()

        # æŠ•èµ„ç†è´¢
        if any(k in title or k in ' '.join(tags) or k in category
               for k in ['æŠ•èµ„', 'ç¾è‚¡', 'å®šæŠ•', 'tqqq', 'qqq', 'æœŸæƒ', 'è‚¡ç¥¨', 'åŸºé‡‘', 'qdii', 'ç†è´¢', 'dca']):
            return 'ğŸ’° æŠ•èµ„ç†è´¢'

        # ç‰¹æ–¯æ‹‰ä¸“é¢˜
        if any(k in title or k in ' '.join(tags)
               for k in ['ç‰¹æ–¯æ‹‰', 'tesla', 'fsd', 'robotaxi', 'optimus']):
            return 'ğŸš— ç‰¹æ–¯æ‹‰ç ”ç©¶'

        # æŠ€æœ¯èµ‹èƒ½
        if any(k in title or k in ' '.join(tags) or k in category
               for k in ['æŠ€æœ¯', 'vps', 'docker', 'éƒ¨ç½²', 'æœåŠ¡å™¨', 'linux', 'python', 'ä»£ç ', 'å¼€å‘', 'selfhosted']):
            return 'ğŸ› ï¸ æŠ€æœ¯èµ‹èƒ½'

        # è®¤çŸ¥å‡çº§
        if any(k in title or k in ' '.join(tags) or k in category
               for k in ['è®¤çŸ¥', 'æ€ç»´', 'å¿ƒç†', 'å­¦ä¹ ', 'æˆé•¿', 'è‡ªæˆ‘å¯¹è¯', 'æ½œæ„è¯†']):
            return 'ğŸ§  è®¤çŸ¥å‡çº§'

        # å…¨çƒè§†é‡
        if any(k in title or k in ' '.join(tags) or k in category
               for k in ['å›½é™…', 'å…¨çƒ', 'ä¸–ç•Œ', 'ç¾å›½', 'æ”¿æ²»', 'ç»æµ', 'youtube', 'æ–°é—»']):
            return 'ğŸŒ å…¨çƒè§†é‡'

        return 'ğŸ“ å…¶ä»–'

    def generate_report(self, duplicates, unique_gridea):
        """ç”Ÿæˆå»é‡æŠ¥å‘Š"""
        output_file = self.project_root / "_drafts/todos/DEDUPLICATED-ARTICLES-REGISTRY.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# ğŸ“Š æ–‡ç« å»é‡åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # ç»Ÿè®¡æ•°æ®
            f.write(f"## ğŸ“ˆ ç»Ÿè®¡æ¦‚è§ˆ\n\n")
            f.write(f"- **Grideaæ€»æ•°**: {len(self.gridea_articles)} ç¯‡\n")
            f.write(f"- **Jekyllæ€»æ•°**: {len(self.jekyll_articles)} ç¯‡\n")
            f.write(f"- **é‡å¤æ–‡ç« **: {len(duplicates)} ç¯‡\n")
            f.write(f"- **Grideaç‹¬æœ‰**: {len(unique_gridea)} ç¯‡\n")
            f.write(f"- **å»é‡ç‡**: {len(duplicates)/len(self.gridea_articles)*100:.1f}%\n\n")

            # é‡å¤æ–‡ç« åˆ—è¡¨
            f.write(f"## âœ… å·²åœ¨Jekyllå‘å¸ƒçš„æ–‡ç«  ({len(duplicates)} ç¯‡)\n\n")

            # æŒ‰åˆ†ç±»æ•´ç†
            categorized_dups = {}
            for dup in duplicates:
                cat = self.categorize_article(dup['gridea'])
                if cat not in categorized_dups:
                    categorized_dups[cat] = []
                categorized_dups[cat].append(dup)

            for cat in sorted(categorized_dups.keys()):
                items = categorized_dups[cat]
                f.write(f"### {cat} ({len(items)}ç¯‡)\n\n")
                for dup in items:
                    gridea = dup['gridea']
                    jekyll = dup['jekyll']
                    f.write(f"- **{gridea['title']}**\n")
                    f.write(f"  - Gridea: {gridea['date']} ({gridea['word_count']:,}å­—)\n")
                    f.write(f"  - Jekyll: `{Path(jekyll['file_path']).name}` ({jekyll['date']})\n\n")

            # Grideaç‹¬æœ‰æ–‡ç« 
            f.write(f"\n## âŒ å°šæœªè¿ç§»çš„Grideaæ–‡ç«  ({len(unique_gridea)} ç¯‡)\n\n")

            # æŒ‰åˆ†ç±»æ•´ç†
            categorized_unique = {}
            for article in unique_gridea:
                cat = self.categorize_article(article)
                if cat not in categorized_unique:
                    categorized_unique[cat] = []
                categorized_unique[cat].append(article)

            for cat in sorted(categorized_unique.keys()):
                items = sorted(categorized_unique[cat], key=lambda x: x['word_count'], reverse=True)

                # è®¡ç®—æ¨èè¿ç§»æ•°é‡
                recommend_count = sum(1 for a in items if a['word_count'] >= 3000 and a['date'] >= '2024-01-01')

                f.write(f"### {cat} ({len(items)}ç¯‡)")
                if recommend_count > 0:
                    f.write(f" - ğŸ”¥ {recommend_count}ç¯‡æ¨èè¿ç§»")
                f.write(f"\n\n")

                for article in items:
                    # åˆ¤æ–­è¿ç§»ä¼˜å…ˆçº§
                    if article['word_count'] >= 3000 and article['date'] >= '2024-01-01':
                        priority = 'ğŸ”¥'
                    elif article['word_count'] >= 2000 and article['date'] >= '2023-01-01':
                        priority = 'ğŸ“'
                    else:
                        priority = 'ğŸ“¦'

                    f.write(f"{priority} **{article['title']}** ({article['word_count']:,}å­—, {article['date']})\n")
                    if article['tags']:
                        f.write(f"   - æ ‡ç­¾: {', '.join(article['tags'][:5])}\n")
                    f.write(f"\n")

            # è¿ç§»å»ºè®®
            f.write(f"\n## ğŸ’¡ è¿ç§»å»ºè®®\n\n")
            f.write(f"### ä¼˜å…ˆçº§è¯´æ˜\n\n")
            f.write(f"- ğŸ”¥ **æ¨èè¿ç§»**: 3,000å­—ä»¥ä¸Šï¼Œ2024å¹´ååˆ›ä½œï¼Œå†…å®¹ä»·å€¼é«˜\n")
            f.write(f"- ğŸ“ **å¯è€ƒè™‘**: 2,000å­—ä»¥ä¸Šï¼Œ2023å¹´ååˆ›ä½œ\n")
            f.write(f"- ğŸ“¦ **å¯å½’æ¡£**: å­—æ•°è¾ƒå°‘æˆ–æ—¶æ•ˆæ€§è¾ƒå¼±\n\n")

            # ç»Ÿè®¡æ¨èè¿ç§»æ•°é‡
            high_priority = [a for a in unique_gridea if a['word_count'] >= 3000 and a['date'] >= '2024-01-01']
            f.write(f"### æ¨èç«‹å³è¿ç§» ({len(high_priority)} ç¯‡)\n\n")
            for article in sorted(high_priority, key=lambda x: x['word_count'], reverse=True):
                cat = self.categorize_article(article)
                f.write(f"- {cat} **{article['title']}** ({article['word_count']:,}å­—)\n")

        print(f"\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
        return output_file

    def run(self):
        """æ‰§è¡Œå»é‡åˆ†æ"""
        print("=" * 60)
        print("ğŸ”„ æ–‡ç« å»é‡åˆ†æå·¥å…·")
        print("=" * 60)

        self.scan_gridea_articles()
        self.scan_jekyll_articles()

        duplicates, unique_gridea = self.find_duplicates()

        print(f"\nğŸ“Š å»é‡ç»“æœ:")
        print(f"  - é‡å¤æ–‡ç« : {len(duplicates)} ç¯‡")
        print(f"  - Grideaç‹¬æœ‰: {len(unique_gridea)} ç¯‡")

        report_file = self.generate_report(duplicates, unique_gridea)

        return report_file

if __name__ == "__main__":
    deduplicator = ArticleDeduplicator()
    deduplicator.run()
