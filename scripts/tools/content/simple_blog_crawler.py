#!/usr/bin/env python3
"""
ç®€å•çš„åšå®¢çˆ¬è™« - ä¸ä¾èµ–BeautifulSoup
ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥ä»HTMLä¸­æå–æ–‡ç« ä¿¡æ¯
"""

import re
import requests
from pathlib import Path
import json
from datetime import datetime


def fetch_page(url):
    """è·å–ç½‘é¡µå†…å®¹"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥: {e}")
        return None


def extract_articles_from_html(html):
    """ä»HTMLä¸­æå–æ–‡ç« ä¿¡æ¯"""
    articles = []

    # Gridea çš„å®é™…HTMLç»“æ„ï¼š
    # <article class="post">
    #   <a href="...">
    #     <h2 class="post-title">æ ‡é¢˜</h2>
    #   </a>
    # </article>

    # æå–æ‰€æœ‰æ–‡ç« å—
    article_pattern = r'<article[^>]*class="post"[^>]*>(.*?)</article>'
    article_blocks = re.findall(article_pattern, html, re.DOTALL)

    for block in article_blocks:
        # æå–é“¾æ¥ï¼ˆåœ¨aæ ‡ç­¾ä¸­ï¼‰
        link_match = re.search(r'<a\s+href="([^"]+)"', block)
        if not link_match:
            continue

        link = link_match.group(1)

        # æå–æ ‡é¢˜ï¼ˆåœ¨h2ä¸­ï¼‰
        title_match = re.search(r'<h2[^>]*class="post-title"[^>]*>(.*?)</h2>', block, re.DOTALL)
        if not title_match:
            continue

        title = re.sub(r'<[^>]+>', '', title_match.group(1)).strip()
        title = re.sub(r'\s+', ' ', title)  # æ¸…ç†å¤šä½™ç©ºæ ¼

        # æå–æ—¥æœŸ
        date_match = re.search(r'<time[^>]*>(.*?)</time>', block)
        date = date_match.group(1).strip() if date_match else 'Unknown'

        # ä»é“¾æ¥ä¸­æå–slug
        slug_match = re.search(r'/post/([^/]+)/?$', link)
        slug = slug_match.group(1) if slug_match else link.split('/')[-1]

        articles.append({
            'title': title,
            'link': link if link.startswith('http') else f"https://zhurong2020.github.io{link}",
            'slug': slug,
            'date': date
        })

    return articles


def crawl_archives():
    """çˆ¬å–å½’æ¡£é¡µé¢"""
    print("ğŸŒ å¼€å§‹çˆ¬å–å½’æ¡£é¡µé¢...")

    url = "https://zhurong2020.github.io/archives/"
    html = fetch_page(url)

    if not html:
        return []

    articles = extract_articles_from_html(html)
    print(f"âœ… ä»å½’æ¡£é¡µé¢æå–åˆ° {len(articles)} ç¯‡æ–‡ç« ")

    return articles


def crawl_homepage_pagination():
    """çˆ¬å–é¦–é¡µçš„åˆ†é¡µå†…å®¹"""
    print("\nğŸŒ å¼€å§‹çˆ¬å–é¦–é¡µåˆ†é¡µ...")
    all_articles = []

    # å°è¯•æŠ“å–å‰5é¡µ
    for page in range(1, 6):
        if page == 1:
            url = "https://zhurong2020.github.io/"
        else:
            url = f"https://zhurong2020.github.io/page/{page}/"

        print(f"  æ­£åœ¨æŠ“å–ç¬¬{page}é¡µ...")
        html = fetch_page(url)

        if not html:
            break

        articles = extract_articles_from_html(html)
        if not articles:
            print(f"  ç¬¬{page}é¡µæ²¡æœ‰æ‰¾åˆ°æ–‡ç« ï¼Œåœæ­¢")
            break

        all_articles.extend(articles)
        print(f"  âœ… ç¬¬{page}é¡µ: {len(articles)} ç¯‡æ–‡ç« ")

    return all_articles


def scan_local_posts():
    """æ‰«ææœ¬åœ°_postsç›®å½•"""
    project_root = Path(__file__).parent.parent.parent.parent
    posts_dir = project_root / "_posts"

    local_articles = []
    for md_file in posts_dir.glob("*.md"):
        filename = md_file.stem
        match = re.match(r'^(\d{4}-\d{2}-\d{2})-(.+)$', filename)
        if match:
            date, slug = match.groups()
            local_articles.append({
                'filename': md_file.name,
                'slug': slug,
                'date': date
            })

    return local_articles


def deduplicate_articles(articles):
    """å»é‡æ–‡ç« ï¼ˆåŸºäºslugï¼‰"""
    unique = {}
    for article in articles:
        slug = article['slug']
        if slug not in unique:
            unique[slug] = article

    return list(unique.values())


def compare_and_report():
    """å¯¹æ¯”çº¿ä¸Šå’Œæœ¬åœ°æ–‡ç« """
    # 1. çˆ¬å–çº¿ä¸Šæ–‡ç« 
    online_articles = []

    # ä»å½’æ¡£é¡µé¢çˆ¬å–
    archives_articles = crawl_archives()
    online_articles.extend(archives_articles)

    # ä»é¦–é¡µåˆ†é¡µçˆ¬å–
    homepage_articles = crawl_homepage_pagination()
    online_articles.extend(homepage_articles)

    # å»é‡
    online_articles = deduplicate_articles(online_articles)
    online_articles.sort(key=lambda x: x.get('date', ''), reverse=True)

    print(f"\nğŸ“Š çº¿ä¸Šæ–‡ç« æ€»æ•°ï¼ˆå»é‡åï¼‰: {len(online_articles)} ç¯‡")

    # 2. æ‰«ææœ¬åœ°æ–‡ç« 
    print("\nğŸ“ æ‰«ææœ¬åœ°_postsç›®å½•...")
    local_articles = scan_local_posts()
    print(f"âœ… æœ¬åœ°æ–‡ç« æ€»æ•°: {len(local_articles)} ç¯‡")

    # 3. å¯¹æ¯”
    print("\nğŸ” å¯¹æ¯”çº¿ä¸Šå’Œæœ¬åœ°æ–‡ç« ...")

    local_slugs = {a['slug'] for a in local_articles}
    online_slugs = {a['slug'] for a in online_articles}

    missing_local = [a for a in online_articles if a['slug'] not in local_slugs]
    not_online = [a for a in local_articles if a['slug'] not in online_slugs]

    # 4. ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š æ–‡ç« å¯¹æ¯”æŠ¥å‘Š")
    print("="*70)

    print(f"\nçº¿ä¸Šæ–‡ç« : {len(online_articles)} ç¯‡")
    print(f"æœ¬åœ°æ–‡ç« : {len(local_articles)} ç¯‡")
    print(f"ä¸¤è€…éƒ½æœ‰: {len(online_slugs & local_slugs)} ç¯‡")

    if missing_local:
        print(f"\nâš ï¸ çº¿ä¸Šæœ‰ä½†æœ¬åœ°ç¼ºå¤±: {len(missing_local)} ç¯‡")
        for article in missing_local[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {article.get('date', 'No date')} | {article['title']}")
            print(f"    é“¾æ¥: {article['link']}")
            print(f"    Slug: {article['slug']}")
        if len(missing_local) > 10:
            print(f"  ... è¿˜æœ‰ {len(missing_local) - 10} ç¯‡")
    else:
        print("\nâœ… æ‰€æœ‰çº¿ä¸Šæ–‡ç« åœ¨æœ¬åœ°éƒ½æœ‰å¯¹åº”")

    if not_online:
        print(f"\nğŸ“ æœ¬åœ°æœ‰ä½†æœªå‘å¸ƒ: {len(not_online)} ç¯‡")
        for article in not_online[:5]:
            print(f"  - {article['date']} | {article['slug']}")

    print("\n" + "="*70)

    # 5. ä¿å­˜è¯¦ç»†æ•°æ®
    project_root = Path(__file__).parent.parent.parent.parent
    output_file = project_root / "_drafts" / "todos" / "crawled_blog_data.json"

    data = {
        'online_articles': online_articles,
        'local_articles': local_articles,
        'missing_local': missing_local,
        'not_online': not_online,
        'generated_at': datetime.now().isoformat(),
        'stats': {
            'online_total': len(online_articles),
            'local_total': len(local_articles),
            'both': len(online_slugs & local_slugs),
            'missing_local': len(missing_local),
            'not_online': len(not_online)
        }
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

    # 6. ç”ŸæˆMarkdownåˆ—è¡¨
    md_output = []
    md_output.append("## ğŸŒ çº¿ä¸Šåšå®¢æ–‡ç« å®Œæ•´æ¸…å•")
    md_output.append(f"\n**çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    md_output.append(f"**æ–‡ç« æ€»æ•°**: {len(online_articles)} ç¯‡\n")

    md_output.append("| åºå· | å‘å¸ƒæ—¥æœŸ | æ–‡ç« æ ‡é¢˜ | Slug | æœ¬åœ°çŠ¶æ€ |")
    md_output.append("|------|---------|---------|------|---------|")

    for idx, article in enumerate(online_articles, 1):
        date = article.get('date', 'Unknown')
        title = article['title']
        slug = article['slug']
        local_status = "âœ… å·²æœ‰" if slug in local_slugs else "âŒ ç¼ºå¤±"

        md_output.append(f"| {idx} | {date} | {title} | `{slug}` | {local_status} |")

    md_file = project_root / "_drafts" / "todos" / "online_blog_articles.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(md_output))

    print(f"ğŸ“„ Markdownåˆ—è¡¨å·²ä¿å­˜åˆ°: {md_file}\n")

    return data


if __name__ == "__main__":
    compare_and_report()
