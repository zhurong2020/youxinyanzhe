#!/usr/bin/env python3
"""
OneDriveåšå®¢å›¾ç‰‡ç´¢å¼•ç®¡ç†ç³»ç»Ÿ
æä¾›å›¾ç‰‡ä¸Šä¼ è®°å½•ã€æŸ¥è¯¢ã€ç®¡ç†å’Œç»Ÿè®¡åŠŸèƒ½
"""

import json
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

@dataclass
class ImageRecord:
    """å›¾ç‰‡è®°å½•æ•°æ®ç»“æ„"""
    # åŸºæœ¬ä¿¡æ¯
    local_path: str                 # æœ¬åœ°åŸå§‹è·¯å¾„
    onedrive_path: str             # OneDriveå­˜å‚¨è·¯å¾„
    onedrive_url: str              # OneDriveåˆ†äº«é“¾æ¥
    embed_url: str                 # åµŒå…¥å¼é“¾æ¥
    
    # æ–‡ç« ä¿¡æ¯
    article_file: str              # æ–‡ç« æ–‡ä»¶è·¯å¾„
    article_title: str             # æ–‡ç« æ ‡é¢˜
    article_date: str              # æ–‡ç« æ—¥æœŸ
    
    # å›¾ç‰‡å±æ€§
    filename: str                  # æ–‡ä»¶å
    file_size: int                 # æ–‡ä»¶å¤§å°(å­—èŠ‚)
    file_hash: str                 # MD5å“ˆå¸Œå€¼(å»é‡)
    image_index: int               # åœ¨æ–‡ç« ä¸­çš„å›¾ç‰‡åºå·
    
    # ä¸Šä¼ ä¿¡æ¯
    upload_date: str               # ä¸Šä¼ æ—¶é—´
    onedrive_file_id: str          # OneDriveæ–‡ä»¶ID
    processing_notes: Optional[str] = None  # å¤„ç†å¤‡æ³¨


class OneDriveImageIndex:
    """OneDriveå›¾ç‰‡ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self, index_file: str = "_data/onedrive_image_index.json"):
        self.index_file = Path(index_file)
        self.index_file.parent.mkdir(exist_ok=True, parents=True)
        self.records: Dict[str, ImageRecord] = {}
        self._load_index()
    
    def _load_index(self):
        """åŠ è½½ç´¢å¼•æ–‡ä»¶"""
        try:
            if self.index_file.exists():
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.records = {
                        key: ImageRecord(**record) 
                        for key, record in data.items()
                    }
                print(f"ğŸ“– åŠ è½½äº† {len(self.records)} æ¡å›¾ç‰‡è®°å½•")
            else:
                print("ğŸ“ åˆ›å»ºæ–°çš„å›¾ç‰‡ç´¢å¼•æ–‡ä»¶")
        except Exception as e:
            print(f"âš ï¸ ç´¢å¼•æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.records = {}
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶"""
        try:
            data = {key: asdict(record) for key, record in self.records.items()}
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ å·²ä¿å­˜ {len(self.records)} æ¡å›¾ç‰‡è®°å½•")
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œå€¼"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶å“ˆå¸Œè®¡ç®—å¤±è´¥ {file_path}: {e}")
            return ""
    
    def add_record(self, 
                   local_path: str,
                   onedrive_path: str,
                   onedrive_url: str,
                   embed_url: str,
                   article_file: str,
                   article_title: str,
                   onedrive_file_id: str,
                   image_index: int = 1,
                   processing_notes: Optional[str] = None) -> str:
        """æ·»åŠ å›¾ç‰‡è®°å½•"""
        
        # ç”Ÿæˆè®°å½•é”®å€¼
        file_hash = self._calculate_file_hash(local_path)
        record_key = f"{Path(article_file).stem}_{image_index:02d}_{file_hash[:8]}"
        
        # æ£€æŸ¥é‡å¤æ–‡ä»¶
        if self.is_duplicate_file(file_hash):
            existing = self.find_by_hash(file_hash)
            print(f"âš ï¸ æ£€æµ‹åˆ°é‡å¤æ–‡ä»¶: {existing.local_path}")
            return existing.embed_url
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        local_file = Path(local_path)
        file_size = local_file.stat().st_size if local_file.exists() else 0
        
        # æå–æ–‡ç« æ—¥æœŸ
        article_date = self._extract_article_date(article_file)
        
        # åˆ›å»ºè®°å½•
        record = ImageRecord(
            local_path=local_path,
            onedrive_path=onedrive_path,
            onedrive_url=onedrive_url,
            embed_url=embed_url,
            article_file=article_file,
            article_title=article_title,
            article_date=article_date,
            filename=local_file.name,
            file_size=file_size,
            file_hash=file_hash,
            image_index=image_index,
            upload_date=datetime.now().isoformat(),
            onedrive_file_id=onedrive_file_id,
            processing_notes=processing_notes
        )
        
        # ä¿å­˜è®°å½•
        self.records[record_key] = record
        self._save_index()
        
        print(f"âœ… å·²æ·»åŠ å›¾ç‰‡è®°å½•: {record_key}")
        return embed_url
    
    def _extract_article_date(self, article_file: str) -> str:
        """ä»æ–‡ç« æ–‡ä»¶åæå–æ—¥æœŸ"""
        try:
            filename = Path(article_file).name
            # åŒ¹é… YYYY-MM-DD æ ¼å¼
            import re
            match = re.search(r'(\d{4}-\d{2}-\d{2})', filename)
            return match.group(1) if match else datetime.now().strftime('%Y-%m-%d')
        except:
            return datetime.now().strftime('%Y-%m-%d')
    
    def is_duplicate_file(self, file_hash: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ–‡ä»¶"""
        return any(record.file_hash == file_hash for record in self.records.values())
    
    def find_by_hash(self, file_hash: str) -> Optional[ImageRecord]:
        """æ ¹æ®å“ˆå¸Œå€¼æŸ¥æ‰¾è®°å½•"""
        for record in self.records.values():
            if record.file_hash == file_hash:
                return record
        return None
    
    def find_by_article(self, article_file: str) -> List[ImageRecord]:
        """æŸ¥æ‰¾æ–‡ç« ç›¸å…³çš„æ‰€æœ‰å›¾ç‰‡"""
        article_stem = Path(article_file).stem
        return [record for record in self.records.values() 
                if Path(record.article_file).stem == article_stem]
    
    def find_by_date_range(self, start_date: str, end_date: str) -> List[ImageRecord]:
        """æŸ¥æ‰¾æ—¥æœŸèŒƒå›´å†…çš„å›¾ç‰‡"""
        results = []
        for record in self.records.values():
            if start_date <= record.article_date <= end_date:
                results.append(record)
        return sorted(results, key=lambda x: x.article_date)
    
    def get_statistics(self) -> Dict:
        """è·å–å›¾ç‰‡ç»Ÿè®¡ä¿¡æ¯"""
        if not self.records:
            return {"total_images": 0, "total_size": 0, "total_articles": 0}
        
        total_size = sum(record.file_size for record in self.records.values())
        articles = set(Path(record.article_file).stem for record in self.records.values())
        
        # æŒ‰æœˆä»½ç»Ÿè®¡
        monthly_stats = {}
        for record in self.records.values():
            month = record.article_date[:7]  # YYYY-MM
            if month not in monthly_stats:
                monthly_stats[month] = {"count": 0, "size": 0}
            monthly_stats[month]["count"] += 1
            monthly_stats[month]["size"] += record.file_size
        
        return {
            "total_images": len(self.records),
            "total_size": total_size,
            "total_size_mb": round(total_size / (1024*1024), 2),
            "total_articles": len(articles),
            "monthly_stats": monthly_stats,
            "average_size_per_image": round(total_size / len(self.records) / 1024, 2) if self.records else 0,
            "latest_upload": max(record.upload_date for record in self.records.values()) if self.records else None
        }
    
    def generate_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        stats = self.get_statistics()
        
        report = [
            "ğŸ“Š OneDriveåšå®¢å›¾ç‰‡ç´¢å¼•æŠ¥å‘Š",
            "=" * 50,
            "",
            "ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:",
            f"  â€¢ å›¾ç‰‡æ€»æ•°: {stats['total_images']} å¼ ",
            f"  â€¢ å­˜å‚¨ç©ºé—´: {stats['total_size_mb']} MB",
            f"  â€¢ æ¶‰åŠæ–‡ç« : {stats['total_articles']} ç¯‡",
            f"  â€¢ å¹³å‡å›¾ç‰‡å¤§å°: {stats['average_size_per_image']} KB",
            f"  â€¢ æœ€è¿‘ä¸Šä¼ : {stats['latest_upload'][:19] if stats['latest_upload'] else 'N/A'}",
            "",
            "ğŸ“… æŒ‰æœˆä»½ç»Ÿè®¡:",
        ]
        
        for month, data in sorted(stats['monthly_stats'].items(), reverse=True):
            size_mb = round(data['size'] / (1024*1024), 2)
            report.append(f"  â€¢ {month}: {data['count']} å¼ å›¾ç‰‡, {size_mb} MB")
        
        return "\n".join(report)
    
    def cleanup_missing_files(self) -> int:
        """æ¸…ç†æœ¬åœ°æ–‡ä»¶å·²åˆ é™¤çš„è®°å½•"""
        to_remove = []
        for key, record in self.records.items():
            if not Path(record.local_path).exists():
                to_remove.append(key)
        
        for key in to_remove:
            del self.records[key]
        
        if to_remove:
            self._save_index()
            print(f"ğŸ§¹ å·²æ¸…ç† {len(to_remove)} æ¡æ— æ•ˆè®°å½•")
        
        return len(to_remove)
    
    def export_urls_for_article(self, article_file: str) -> Dict[str, str]:
        """å¯¼å‡ºæ–‡ç« ç›¸å…³çš„æ‰€æœ‰å›¾ç‰‡URLæ˜ å°„"""
        records = self.find_by_article(article_file)
        return {
            record.local_path: record.embed_url 
            for record in sorted(records, key=lambda x: x.image_index)
        }


def main():
    """å‘½ä»¤è¡Œå·¥å…·"""
    import argparse
    
    parser = argparse.ArgumentParser(description='OneDriveå›¾ç‰‡ç´¢å¼•ç®¡ç†')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š')
    parser.add_argument('--cleanup', action='store_true', help='æ¸…ç†æ— æ•ˆè®°å½•')
    parser.add_argument('--article', help='æŸ¥çœ‹æŒ‡å®šæ–‡ç« çš„å›¾ç‰‡')
    parser.add_argument('--date-range', nargs=2, help='æŸ¥çœ‹æ—¥æœŸèŒƒå›´å†…çš„å›¾ç‰‡ (å¼€å§‹æ—¥æœŸ ç»“æŸæ—¥æœŸ)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index = OneDriveImageIndex()
    
    if args.stats:
        stats = index.get_statistics()
        print(f"ğŸ“Š æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
        print(f"ğŸ“¦ æ€»å¤§å°: {stats.get('total_size_mb', 0)} MB")
        print(f"ğŸ“ æ¶‰åŠæ–‡ç« : {stats['total_articles']} ç¯‡")
    
    elif args.report:
        print(index.generate_report())
    
    elif args.cleanup:
        cleaned = index.cleanup_missing_files()
        print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned} æ¡è®°å½•")
    
    elif args.article:
        records = index.find_by_article(args.article)
        if records:
            print(f"ğŸ“– æ–‡ç«  {args.article} çš„å›¾ç‰‡:")
            for record in sorted(records, key=lambda x: x.image_index):
                print(f"  {record.image_index:02d}. {record.filename} -> {record.embed_url}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°æ–‡ç«  {args.article} çš„å›¾ç‰‡è®°å½•")
    
    elif args.date_range:
        start_date, end_date = args.date_range
        records = index.find_by_date_range(start_date, end_date)
        print(f"ğŸ“… {start_date} è‡³ {end_date} æœŸé—´çš„å›¾ç‰‡:")
        for record in records:
            print(f"  {record.article_date} - {record.article_title} - {record.filename}")
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()