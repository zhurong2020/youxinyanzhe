"""
è‡ªåŠ¨è§„èŒƒåŒ–å¼•æ“
ç”¨äºä¸€é”®è§„èŒƒåŒ–å’Œæ™ºèƒ½å‘å¸ƒçš„ç»Ÿä¸€è§„èŒƒåŒ–é€»è¾‘
"""

from pathlib import Path
from typing import Dict, Any, List, Optional
import re


class AutoNormalizer:
    """è‡ªåŠ¨è§„èŒƒåŒ–å¼•æ“"""

    # å››å¤§åˆ†ç±»æ˜ å°„
    CATEGORIES_MAP = {
        'cognitive-upgrade': {
            'keywords': ['è®¤çŸ¥', 'æ€ç»´', 'å­¦ä¹ ', 'å¿ƒç†', 'æˆé•¿', 'æ¨¡å‹', 'æ–¹æ³•è®º', 'æ•ˆç‡', 'æå‡'],
            'emoji': 'ğŸ§ ',
            'name_cn': 'è®¤çŸ¥å‡çº§',
            'name_en': 'cognitive-upgrade'
        },
        'tech-empowerment': {
            'keywords': ['æŠ€æœ¯', 'å·¥å…·', 'è‡ªåŠ¨åŒ–', 'ç¼–ç¨‹', 'ä»£ç ', 'AI', 'äººå·¥æ™ºèƒ½', 'è½¯ä»¶', 'åº”ç”¨', 'æ•™ç¨‹'],
            'emoji': 'ğŸ› ï¸',
            'name_cn': 'æŠ€æœ¯èµ‹èƒ½',
            'name_en': 'tech-empowerment'
        },
        'global-perspective': {
            'keywords': ['å…¨çƒ', 'å›½é™…', 'æ–‡åŒ–', 'è¶‹åŠ¿', 'è§†é‡', 'ä¸–ç•Œ', 'è·¨æ–‡åŒ–', 'å·®å¼‚'],
            'emoji': 'ğŸŒ',
            'name_cn': 'å…¨çƒè§†é‡',
            'name_en': 'global-perspective'
        },
        'investment-finance': {
            'keywords': ['æŠ•èµ„', 'ç†è´¢', 'é‡‘è', 'è‚¡ç¥¨', 'åŸºé‡‘', 'é‡åŒ–', 'è´¢å¯Œ', 'èµ„äº§', 'æ”¶ç›Š', 'ç­–ç•¥', 'ç¾è‚¡', 'å®šæŠ•'],
            'emoji': 'ğŸ’°',
            'name_cn': 'æŠ•èµ„ç†è´¢',
            'name_en': 'investment-finance'
        }
    }

    def __init__(self, ai_client=None):
        """
        åˆå§‹åŒ–è‡ªåŠ¨è§„èŒƒåŒ–å¼•æ“

        Args:
            ai_client: AIå®¢æˆ·ç«¯ï¼ˆç”¨äºç”Ÿæˆæ‘˜è¦ï¼‰
        """
        self.ai_client = ai_client

    def infer_category_from_content(self, title: str, tags: List[str], content: str = "") -> Optional[str]:
        """
        åŸºäºæ ‡é¢˜ã€æ ‡ç­¾å’Œå†…å®¹æ¨æ–­åˆ†ç±»

        Args:
            title: æ–‡ç« æ ‡é¢˜
            tags: æ ‡ç­¾åˆ—è¡¨
            content: æ–‡ç« å†…å®¹ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ¨æ–­çš„åˆ†ç±»ï¼ˆè‹±æ–‡keyï¼‰æˆ–None
        """
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ç”¨äºåˆ†æ
        text_to_analyze = f"{title} {' '.join(tags)} {content[:500]}"
        text_lower = text_to_analyze.lower()

        # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„åŒ¹é…åˆ†æ•°
        scores = {}
        for category_key, category_info in self.CATEGORIES_MAP.items():
            score = 0
            for keyword in category_info['keywords']:
                # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
                if keyword in title.lower():
                    score += 3
                # æ ‡ç­¾åŒ¹é…æƒé‡ä¸­ç­‰
                if any(keyword in tag.lower() for tag in tags):
                    score += 2
                # å†…å®¹åŒ¹é…æƒé‡è¾ƒä½
                if keyword in text_lower:
                    score += 1
            scores[category_key] = score

        # è¿”å›å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
        if scores:
            best_category = max(scores.items(), key=lambda x: x[1])
            if best_category[1] > 0:  # è‡³å°‘æœ‰ä¸€ä¸ªå…³é”®è¯åŒ¹é…
                return best_category[0]

        return None

    def optimize_excerpt_with_ai(self, content: str, target_length: int = 70) -> Optional[str]:
        """
        ä½¿ç”¨AIä¼˜åŒ–æ‘˜è¦

        Args:
            content: æ–‡ç« å†…å®¹
            target_length: ç›®æ ‡é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰

        Returns:
            ä¼˜åŒ–åçš„æ‘˜è¦æˆ–None
        """
        if not self.ai_client:
            return None

        try:
            # æå–æ–‡ç« å¼€å¤´éƒ¨åˆ†ç”¨äºAIåˆ†æ
            # ç§»é™¤Front Matter
            content_lines = content.split('\n')
            fm_count = 0
            content_start = 0
            for i, line in enumerate(content_lines):
                if line.strip() == '---':
                    fm_count += 1
                    if fm_count == 2:
                        content_start = i + 1
                        break

            main_content = '\n'.join(content_lines[content_start:content_start+50])  # å‰50è¡Œ

            prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ª{target_length}å­—ç¬¦å·¦å³çš„ç²¾ç‚¼æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. å‡†ç¡®æ¦‚æ‹¬æ–‡ç« æ ¸å¿ƒå†…å®¹
2. é•¿åº¦ä¸¥æ ¼æ§åˆ¶åœ¨{target_length}å­—ç¬¦å·¦å³ï¼ˆä¸è¶…è¿‡{target_length + 10}å­—ç¬¦ï¼‰
3. è¯­è¨€ç®€æ´æœ‰åŠ›ï¼Œå¸å¼•è¯»è€…
4. ä¸è¦ä½¿ç”¨"æœ¬æ–‡"ã€"è¿™ç¯‡æ–‡ç« "ç­‰å¼€å¤´
5. ç›´æ¥ä»å†…å®¹æ ¸å¿ƒå¼€å§‹

æ–‡ç« å†…å®¹ï¼š
{main_content}

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸è¦å…¶ä»–è¯´æ˜ï¼š"""

            # è°ƒç”¨AIç”Ÿæˆæ‘˜è¦
            response = self.ai_client.generate_content(prompt)
            if response and hasattr(response, 'text'):
                summary = response.text.strip()
                # ç¡®ä¿é•¿åº¦åˆé€‚
                if len(summary) > target_length + 15:
                    summary = summary[:target_length] + "..."
                return summary

        except Exception as e:
            print(f"   âš ï¸ AIç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return None

        return None

    def truncate_excerpt(self, excerpt: str, max_length: int = 80) -> str:
        """
        æˆªæ–­æ‘˜è¦åˆ°æŒ‡å®šé•¿åº¦

        Args:
            excerpt: åŸå§‹æ‘˜è¦
            max_length: æœ€å¤§é•¿åº¦

        Returns:
            æˆªæ–­åçš„æ‘˜è¦
        """
        if len(excerpt) <= max_length:
            return excerpt

        # åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        truncated = excerpt[:max_length]

        # æ‰¾åˆ°æœ€åä¸€ä¸ªå¥å­ç»“æŸç¬¦
        last_period = max(
            truncated.rfind('ã€‚'),
            truncated.rfind('ï¼'),
            truncated.rfind('ï¼Ÿ'),
            truncated.rfind('.'),
            truncated.rfind('!'),
            truncated.rfind('?')
        )

        if last_period > max_length * 0.7:  # å¦‚æœå¥å­è¾¹ç•Œä¸å¤ªè¿œ
            return truncated[:last_period + 1]
        else:
            # å¦åˆ™åœ¨æœ€åä¸€ä¸ªé€—å·å¤„æˆªæ–­
            last_comma = max(truncated.rfind('ï¼Œ'), truncated.rfind(','))
            if last_comma > max_length * 0.7:
                return truncated[:last_comma] + "..."
            else:
                return truncated + "..."

    def extract_front_matter(self, content: str) -> tuple[Dict[str, Any], str]:
        """
        æå–Front Matterå’Œæ­£æ–‡å†…å®¹

        Args:
            content: æ–‡ä»¶å†…å®¹

        Returns:
            (front_matter_dict, body_content)
        """
        import frontmatter

        try:
            post = frontmatter.loads(content)
            return dict(post.metadata), post.content
        except Exception as e:
            print(f"   âš ï¸ è§£æFront Matterå¤±è´¥: {e}")
            return {}, content

    def rebuild_content(self, front_matter: Dict[str, Any], body: str) -> str:
        """
        é‡å»ºæ–‡ä»¶å†…å®¹ï¼ˆFront Matter + æ­£æ–‡ï¼‰

        Args:
            front_matter: Front Matterå­—å…¸
            body: æ­£æ–‡å†…å®¹

        Returns:
            å®Œæ•´æ–‡ä»¶å†…å®¹
        """
        import yaml

        fm_str = yaml.dump(front_matter, allow_unicode=True, default_flow_style=False, sort_keys=False)
        return f"---\n{fm_str}---\n{body}"
